# Sample Logstash configuration for creating a simple
# Beats -> Logstash -> Elasticsearch pipeline.

# input {
#  http {
#     host => "0.0.0.0"
#     port => "8080"
#   }
# }

# input {
#   beats{port => 5044} 
# }

input {
  kafka {
    bootstrap_servers => "kafka01:9092"
    topics => ["filebeat"]
    codec => json
  }
}

filter {
  csv {
    separator => ","
    columns => ["vm_id","subscription_id","deployment_id","timestamp_vm_created","timestamp_vm_deleted","max_cpu", "average_cpu" ,"p96_max_cpu","vm_category","vm_virtual_core_count_bucket","vm_memory_bucket"]
  }

  mutate {
    convert => {
      "timestamp_vm_created" => "integer"
      "timestamp_vm_deleted" => "integer"
      "max_cpu" => "float"
      "average_cpu" => "float"
      "p96_max_cpu" => "float"
      "vm_virtual_core_count_bucket" => "integer"
      "vm_memory_bucket" => "integer"
    }

    remove_field => [ "message", "agent", "host", "input", "event", "log", "ecs", "tags", "@version" ]

  }

}

# output {
#   stdout { codec => rubydebug }
# }


output {
  elasticsearch {
    hosts => ["http://es01:9200"]
    index => "vmtable"
  }
}
